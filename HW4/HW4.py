

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')
  

  
# Set style cho ƒë·ªì th·ªã
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10

# Set random seed
np.random.seed(42)


def generate_housing_data(n_samples=200):
    """
    T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p v·ªÅ gi√° nh√† v·ªõi nhi·ªÅu features
    
    Args:
        n_samples: S·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu
        
    Returns:
        DataFrame ch·ª©a t·∫•t c·∫£ d·ªØ li·ªáu
    """
    # T·∫°o features
    dien_tich = np.random.uniform(30, 300, n_samples)  # 30-300 m¬≤
    so_phong_ngu = np.random.randint(1, 6, n_samples)  # 1-5 ph√≤ng
    so_phong_tam = np.random.randint(1, 4, n_samples)  # 1-3 ph√≤ng
    tuoi_nha = np.random.uniform(0, 50, n_samples)     # 0-50 nƒÉm
    
    # T·∫°o gi√° nh√† v·ªõi c√¥ng th·ª©c ph·ª©c t·∫°p h∆°n
    # Gi√° = 40*di·ªán_t√≠ch + 800*ph√≤ng_ng·ªß + 500*ph√≤ng_t·∫Øm - 10*tu·ªïi_nh√† + noise
    noise = np.random.normal(0, 1000, n_samples)
    gia_nha = (40 * dien_tich + 
               800 * so_phong_ngu + 
               500 * so_phong_tam - 
               10 * tuoi_nha + 
               2000 +  # Base price
               noise)
    
    # T·∫°o DataFrame
    df = pd.DataFrame({
        'Dien_tich_m2': dien_tich,
        'So_phong_ngu': so_phong_ngu,
        'So_phong_tam': so_phong_tam,
        'Tuoi_nha_nam': tuoi_nha,
        'Gia_nha_trieu_VND': gia_nha
    })
    
    return df


def plot_data_analysis(df):
    """
    V·∫Ω ƒë·ªì th·ªã ph√¢n t√≠ch d·ªØ li·ªáu ban ƒë·∫ßu
    """
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    fig.suptitle('PH√ÇN T√çCH D·ªÆ LI·ªÜU BAN ƒê·∫¶U', fontsize=16, fontweight='bold')
    
    # Distribution c·ªßa t·ª´ng feature
    features = ['Dien_tich_m2', 'So_phong_ngu', 'So_phong_tam', 'Tuoi_nha_nam']
    for idx, feature in enumerate(features):
        row = idx // 2
        col = idx % 2
        axes[row, col].hist(df[feature], bins=30, alpha=0.7, color=sns.color_palette()[idx], edgecolor='black')
        axes[row, col].set_xlabel(feature, fontsize=11)
        axes[row, col].set_ylabel('T·∫ßn s·ªë', fontsize=11)
        axes[row, col].set_title(f'Ph√¢n ph·ªëi {feature}', fontweight='bold')
        axes[row, col].grid(True, alpha=0.3)
    
    # Distribution c·ªßa target
    axes[1, 0].hist(df['Gia_nha_trieu_VND'], bins=30, alpha=0.7, color='orange', edgecolor='black')
    axes[1, 0].set_xlabel('Gi√° nh√† (tri·ªáu VNƒê)', fontsize=11)
    axes[1, 0].set_ylabel('T·∫ßn s·ªë', fontsize=11)
    axes[1, 0].set_title('Ph√¢n ph·ªëi Gi√° Nh√†', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Correlation heatmap
    axes[1, 1].axis('off')
    ax_corr = fig.add_subplot(2, 3, 6)
    correlation = df.corr()
    sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, square=True, ax=ax_corr, cbar_kws={'shrink': 0.8})
    ax_corr.set_title('Ma tr·∫≠n Correlation', fontweight='bold')
    
    plt.tight_layout()
    plt.show()


def plot_results(X_test, y_test, y_pred, model, feature_names):
    """
    V·∫Ω ƒë·ªì th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n
    """
    fig = plt.figure(figsize=(16, 10))
    
    # Plot 1: Predicted vs Actual
    ax1 = plt.subplot(2, 3, 1)
    ax1.scatter(y_test, y_pred, alpha=0.6, s=50, color='blue', edgecolor='black', linewidth=0.5)
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
    ax1.set_xlabel('Gi√° th·ª±c t·∫ø (tri·ªáu VNƒê)', fontsize=11)
    ax1.set_ylabel('Gi√° d·ª± ƒëo√°n (tri·ªáu VNƒê)', fontsize=11)
    ax1.set_title('Predicted vs Actual', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Residuals
    ax2 = plt.subplot(2, 3, 2)
    residuals = y_test - y_pred
    ax2.scatter(y_pred, residuals, alpha=0.6, s=50, color='green', edgecolor='black', linewidth=0.5)
    ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
    ax2.set_xlabel('Gi√° d·ª± ƒëo√°n (tri·ªáu VNƒê)', fontsize=11)
    ax2.set_ylabel('Residuals', fontsize=11)
    ax2.set_title('Residual Plot', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Distribution of Residuals
    ax3 = plt.subplot(2, 3, 3)
    ax3.hist(residuals, bins=30, alpha=0.7, color='purple', edgecolor='black')
    ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)
    ax3.set_xlabel('Residuals', fontsize=11)
    ax3.set_ylabel('T·∫ßn s·ªë', fontsize=11)
    ax3.set_title('Ph√¢n ph·ªëi Residuals', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Plot 4: Feature Importance (Coefficients)
    ax4 = plt.subplot(2, 3, 4)
    coefficients = pd.Series(model.coef_, index=feature_names).sort_values()
    colors = ['red' if x < 0 else 'green' for x in coefficients]
    coefficients.plot(kind='barh', ax=ax4, color=colors, edgecolor='black')
    ax4.set_xlabel('Coefficient Value', fontsize=11)
    ax4.set_title('Feature Importance (Coefficients)', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='x')
    
    # Plot 5: Error Distribution
    ax5 = plt.subplot(2, 3, 5)
    errors = np.abs(residuals)
    ax5.hist(errors, bins=30, alpha=0.7, color='orange', edgecolor='black')
    ax5.set_xlabel('Absolute Error (tri·ªáu VNƒê)', fontsize=11)
    ax5.set_ylabel('T·∫ßn s·ªë', fontsize=11)
    ax5.set_title('Ph√¢n ph·ªëi Absolute Error', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    
    # Plot 6: Q-Q Plot
    ax6 = plt.subplot(2, 3, 6)
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=ax6)
    ax6.set_title('Q-Q Plot (Residuals)', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def compare_models(X_train, X_test, y_train, y_test):
    """
    So s√°nh nhi·ªÅu models kh√°c nhau
    """
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge (Œ±=1.0)': Ridge(alpha=1.0),
        'Ridge (Œ±=10.0)': Ridge(alpha=10.0),
        'Lasso (Œ±=0.1)': Lasso(alpha=0.1),
        'Lasso (Œ±=1.0)': Lasso(alpha=1.0)
    }
    
    results = []
    
    for name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        
        # Predict
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # Evaluate
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        
        results.append({
            'Model': name,
            'Train MSE': train_mse,
            'Test MSE': test_mse,
            'Train R¬≤': train_r2,
            'Test R¬≤': test_r2
        })
    
    return pd.DataFrame(results)


def main():
    """
    H√†m main ƒë·ªÉ ch·∫°y to√†n b·ªô ch∆∞∆°ng tr√¨nh
    """
    print("="*90)
    print(" "*15 + "LINEAR REGRESSION - D·ª∞ ƒêO√ÅN GI√Å NH√Ä S·ª¨ D·ª§NG SKLEARN")
    print("="*90)
    print()
    
    # 1. T·∫°o v√† ph√¢n t√≠ch d·ªØ li·ªáu
    print("üìä B∆Ø·ªöC 1: T·∫°o v√† Ph√¢n T√≠ch D·ªØ Li·ªáu")
    print("-" * 90)
    df = generate_housing_data(n_samples=200)
    print(f"‚úì ƒê√£ t·∫°o {len(df)} m·∫´u d·ªØ li·ªáu")
    print(f"\nüìã Th·ªëng k√™ m√¥ t·∫£:")
    print(df.describe().round(2))
    print(f"\nüîç Th√¥ng tin d·ªØ li·ªáu:")
    print(df.info())
    print()
    
    # Visualize d·ªØ li·ªáu ban ƒë·∫ßu
    print("üìä ƒêang v·∫Ω ƒë·ªì th·ªã ph√¢n t√≠ch d·ªØ li·ªáu...")
    plot_data_analysis(df)
    
    # 2. Chu·∫©n b·ªã d·ªØ li·ªáu
    print("\nüîß B∆Ø·ªöC 2: Chu·∫©n B·ªã D·ªØ Li·ªáu")
    print("-" * 90)
    
    # T√°ch features v√† target
    X = df.drop('Gia_nha_trieu_VND', axis=1)
    y = df['Gia_nha_trieu_VND']
    feature_names = X.columns.tolist()
    
    # Chia train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"‚úì Training set: {len(X_train)} m·∫´u ({len(X_train)/len(df)*100:.1f}%)")
    print(f"‚úì Test set: {len(X_test)} m·∫´u ({len(X_test)/len(df)*100:.1f}%)")
    print(f"‚úì S·ªë features: {X.shape[1]}")
    print(f"‚úì Features: {', '.join(feature_names)}")
    print()
    
    # 3. Hu·∫•n luy·ªán model
    print("ü§ñ B∆Ø·ªöC 3: Hu·∫•n Luy·ªán Linear Regression Model (Sklearn)")
    print("-" * 90)
    
    # T·∫°o pipeline v·ªõi scaling
    model = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', LinearRegression())
    ])
    
    # Train model
    model.fit(X_train, y_train)
    print("‚úì Model ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng!")
    
    # L·∫•y coefficients
    regressor = model.named_steps['regressor']
    print(f"\nÔøΩ Model Parameters:")
    print(f"  - Intercept (b): {regressor.intercept_:.4f}")
    print(f"  - Coefficients (w):")
    for fname, coef in zip(feature_names, regressor.coef_):
        print(f"      {fname:20s}: {coef:10.4f}")
    print()
    
    # 4. ƒê√°nh gi√° tr√™n Training Set
    print("üìà B∆Ø·ªöC 4: ƒê√°nh Gi√° Model tr√™n Training Set")
    print("-" * 90)
    y_train_pred = model.predict(X_train)
    
    train_mse = mean_squared_error(y_train, y_train_pred)
    train_rmse = np.sqrt(train_mse)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    
    print(f"  ‚úì MSE (Mean Squared Error): {train_mse:,.2f}")
    print(f"  ‚úì RMSE (Root MSE): {train_rmse:,.2f}")
    print(f"  ‚úì MAE (Mean Absolute Error): {train_mae:,.2f}")
    print(f"  ‚úì R¬≤ Score: {train_r2:.4f} ({train_r2*100:.2f}% variance explained)")
    print()
    
    # 5. ƒê√°nh gi√° tr√™n Test Set
    print("üéØ B∆Ø·ªöC 5: ƒê√°nh Gi√° Model tr√™n Test Set")
    print("-" * 90)
    y_test_pred = model.predict(X_test)
    
    test_mse = mean_squared_error(y_test, y_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"  ‚úì MSE (Mean Squared Error): {test_mse:,.2f}")
    print(f"  ‚úì RMSE (Root MSE): {test_rmse:,.2f}")
    print(f"  ‚úì MAE (Mean Absolute Error): {test_mae:,.2f}")
    print(f"  ‚úì R¬≤ Score: {test_r2:.4f} ({test_r2*100:.2f}% variance explained)")
    print()
    
    # 6. Cross-validation
    print("üîÑ B∆Ø·ªöC 6: Cross-Validation (5-fold)")
    print("-" * 90)
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    print(f"  ‚úì CV R¬≤ Scores: {cv_scores}")
    print(f"  ‚úì Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print()
    
    # 7. So s√°nh c√°c models
    print("‚öñÔ∏è  B∆Ø·ªöC 7: So S√°nh C√°c Models Kh√°c Nhau")
    print("-" * 90)
    comparison_df = compare_models(X_train, X_test, y_train, y_test)
    print(comparison_df.to_string(index=False))
    print()
    
    # 8. D·ª± ƒëo√°n m·∫´u
    print("ÔøΩ B∆Ø·ªöC 8: Th·ª≠ Nghi·ªám D·ª± ƒêo√°n")
    print("-" * 90)
    
    # T·∫°o c√°c m·∫´u test
    test_samples = pd.DataFrame({
        'Dien_tich_m2': [50, 100, 150, 200],
        'So_phong_ngu': [2, 3, 4, 5],
        'So_phong_tam': [1, 2, 2, 3],
        'Tuoi_nha_nam': [5, 10, 20, 0]
    })
    
    predictions = model.predict(test_samples)
    
    print(f"{'DT(m¬≤)':<10} {'Ph√≤ng ng·ªß':<12} {'Ph√≤ng t·∫Øm':<12} {'Tu·ªïi(nƒÉm)':<12} {'Gi√° d·ª± ƒëo√°n (tri·ªáu VNƒê)':<25}")
    print("-" * 90)
    for idx, row in test_samples.iterrows():
        print(f"{row['Dien_tich_m2']:<10.0f} {row['So_phong_ngu']:<12.0f} "
              f"{row['So_phong_tam']:<12.0f} {row['Tuoi_nha_nam']:<12.0f} "
              f"{predictions[idx]:<25,.2f}")
    print()
    
    # 9. Ph√¢n t√≠ch l·ªói
    print("ÔøΩ B∆Ø·ªöC 9: Ph√¢n T√≠ch L·ªói")
    print("-" * 90)
    residuals = y_test - y_test_pred
    print(f"  ‚úì Mean Residual: {residuals.mean():.2f}")
    print(f"  ‚úì Std Residual: {residuals.std():.2f}")
    print(f"  ‚úì Min Residual: {residuals.min():.2f}")
    print(f"  ‚úì Max Residual: {residuals.max():.2f}")
    
    # T√¨m predictions t·ªët nh·∫•t v√† t·ªá nh·∫•t
    abs_errors = np.abs(residuals)
    best_idx = abs_errors.idxmin()
    worst_idx = abs_errors.idxmax()
    
    print(f"\n  üåü D·ª± ƒëo√°n t·ªët nh·∫•t:")
    print(f"     True: {y_test.loc[best_idx]:.2f}, Predicted: {y_test_pred[y_test.index.get_loc(best_idx)]:.2f}, Error: {abs_errors.loc[best_idx]:.2f}")
    print(f"  ‚ö†Ô∏è  D·ª± ƒëo√°n t·ªá nh·∫•t:")
    print(f"     True: {y_test.loc[worst_idx]:.2f}, Predicted: {y_test_pred[y_test.index.get_loc(worst_idx)]:.2f}, Error: {abs_errors.loc[worst_idx]:.2f}")
    print()
    
    # 10. Visualize k·∫øt qu·∫£
    print("üìä B∆Ø·ªöC 10: Visualize K·∫øt Qu·∫£ D·ª± ƒêo√°n")
    print("-" * 90)
    print("ƒêang v·∫Ω ƒë·ªì th·ªã...")
    plot_results(X_test, y_test, y_test_pred, regressor, feature_names)
    
    # K·∫øt lu·∫≠n
    print()
    print("="*90)
    print(" "*30 + "HO√ÄN TH√ÄNH!")
    print("="*90)
    print()
    print("üí° K·∫æT LU·∫¨N:")
    print(f"   ‚úì Model Linear Regression v·ªõi {X.shape[1]} features")
    print(f"   ‚úì R¬≤ Score tr√™n test set: {test_r2:.4f} - Model gi·∫£i th√≠ch ƒë∆∞·ª£c {test_r2*100:.2f}% variance")
    print(f"   ‚úì RMSE: {test_rmse:,.2f} tri·ªáu VNƒê - Sai s·ªë trung b√¨nh")
    print(f"   ‚úì Feature quan tr·ªçng nh·∫•t: {feature_names[np.argmax(np.abs(regressor.coef_))]}")
    print("   ‚úì S·ª≠ d·ª•ng th∆∞ vi·ªán Sklearn gi√∫p code ng·∫Øn g·ªçn, d·ªÖ maintain v√† c√≥ nhi·ªÅu t√≠nh nƒÉng")
    print("="*90)


if __name__ == "__main__":
    main()
